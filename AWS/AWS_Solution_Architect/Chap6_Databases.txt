Dababases 101
    1. Relastional databases on AWS
        -> runs on virtual machines
        -> cannot ssh (or remoting)
        -> cannot login to patching
            RDS vs EC2 patching responsibility:
                -> RDS: AWS responible to path both OS and database
                -> EC2: share responsibility 
        -> RDS is NOT serverless (Aurora is serverless)
        Supported DB:
            -> SQL Server
            -> Oracle
            -> MySQL Server (what we use for this lab)
            -> PostgreSQL
            -> Aurora
            -> MariaDB
        Types: Multiple AZs vs Read Replicas
            - Multi-AZ - to have an exact copy of the production database in another AZ
                + AWS handles replication: 
                    -> writting on production database will automatically be synchronized to the 2nd/stand-by database
                + failover when:
                    -> database maintenance
                    -> database instance failure
                    -> AZ failure
                + failover how:
                    -> AWS will automatically update the IP address to the stand-by database - still using
                        the current DNS string (AWS will handle failover)
                    -> operation can resume quickly - less downtime
                    -> requires no administrative intervention
                + why:
                    -> Disaster Recovery for HA DB
                    -> Good for important DB
                + Failover automatically:
                    + EC2 connect to Primary DB via connecting DNS string 
                    + When Primary DB is down, AWS will automatically create/update another connecting DNS string from EC2 to 
                        the Secondary DB
                + Supported database: everything but Aurora (Aurora has its own faul-tolerence)
                    SQL Server, Oracle, MySQL Server, PostgreSQL, MariaDB
            - Read Replicas - create multiple RDS replica to handle high read requests
                + High Read requests:
                    -> Some database has a lot of Read traffic that can bottle neck its resource/network
                + Solution:
                    -> Replicate the Primary database to multiple read-only database instances - Asynchronous Replication
                    -> Redirect all the Read Requests pan out on all the Read-Replicas database instances
                    -> Only write to the Primary so that it can be replicated our to Replicas - Asynchronous Replication
                -> used for scaling not recovery
                -> must have automatic backups turned on to deploy a read replica
                -> can have up to 5 read replica copies per database
                -> can have Read-replica of Read-replica (watchout for latency)
                -> Each RR has its own DNS endpoint (unlike Multi-AZ)
                -> RR can be Multi-AZ
                -> RR can be created on Multi-AZ databases
                -> RR can be promoted to be their own data but replication will be breaks
                -> RR can be created in second Region
                -> Failover isn't auto:
                    + EC2 connect to Primary DB via connecting DNS string 
                    + When Primary is down, user have to create new URL + manually update DNS on Read Replicas DB
    2. Non Relational Databases
        -> 
    3. Relational DB vs Non-relational DB
    4. Data Warehousing
        -> use for Business Intelligence BI
        -> use to pull in large complex data sets
        -> for management do queries on data
        Online Transaction Processing (OLTP) vs Online Analytics Processing (OLAP)
            OLTP:
                -> small, specific query
                -> contain specific product ID, personal info, etc.
            OLAP:
                -> big, complex number of queries
                -> pull in large number of records
        -> use different type of Architecture both from database perspective and infrastructure Layer
        -> AWS Redshift



RDS - Relational Database Service
    Backups - Automated Backups vs Database snapshots
        Automated Backups:
            -> allow to recover db to any point in time within Retention Period
            -> Retention Period 1-35 days
            -> How:
                -> Automated Backups take: full daily snapshots + store daily transaction logs
                -> recover: choose the most recent daily backup -> apply transaction logs 
            -> allow to recover a point in time recovery down to a second
            -> enable by default
            -> backups store in S3 for free
            -> S3 size = DB size
            -> during backups window, I/O may suspend or elevated latency
        Snapshots
            -> manual (not automate)
            -> save after DB is deleted
            -> restoring a snapshot will be restored on a new RDS instance with a new DNS endpoint
    Encryption At Rest
        - Supported on:
            + MySQL
            + Oracle
            + SQL Server
            + PostgreSQL
            + MariaDB
            + Aurora
        - using Key Management Service (KMS)
        - once RDS instance is encrypted -> all data stored at rest underlying storage is also encrypted:
            + automated backup
            + read Replicas
            + snapshots 

Dynamo DB - AWS Non Relational Database
    What is Dynamo DB?
        - Fast and flexible NoSQL database service
        - For apps needs: consistent, single-digit millisecond latency
        - fully managed database
        - support document models
        - support key-value models
        - flexible + reliable performance
        - for: mobile, web, gaming, ad-tech, IoT, etc.
    -> store on SSD
    -> spread across 3 geographically disctint data center
    -> Eventual Consistent Read: read after write within 1 second - Best read performance
    -> Strongly Consistent Read:  read after write less than 1 second

Advanced DynamoDB
    DynamoDB Accelerator (DAX)
        -> managed, HA, in-memory cache
        -> 10x performance improvement compares to using DynamoDB
        -> reduce request time from milliseconds to microseconds (even under load)
        -> no need to managed caching locagical
        -> combatiple with exist DynamoDB APIs call
        - Traditional caching vs DAX caching:
            -> Traditional:
                + Cache sit off to the side of the application
                + Have to be managed
                + Only support read
            -> DAX:
                + Cache sit between application and DynamoDB
                + No need to be managed
                + Write-through-cache support write
                + HA

    Transactions
        The problems: In all-for-nothing operations, some requests have to happen at the same time
            simutaneously where one fail to happen the other will also fail to happen
        Example: Financial transactions (credit-debit), Fullfilling orders (E-commerce)
        The solutions: 
            -> DynamoDB will perform read or write -> prepare/commit
            -> consume more capacity
            -> up to 25 items or 4MB of data

    On-Demand Capacity
        Pay-per-request pricing vs Provisioning pricing
            Per-per-request:
                Only pay what you use
                Easy to balance cost and performance
                No minimum capacity
            Provisioning
                Change on storage and backups - no charge for read-write
        -> Pay-per-request is more expensive, use for new product launches, when it stable, change to Provisioning

    On-Demand Backup and Restore
        -> Full backups at anytime
        -> zero impact on table performance and availability (unlike RDS)
        -> consistent within seconds + retained until deleted
        -> operate within same region only
        
        Point-in-Time Recovery (PITR)
            -> to protect agaisnt accidental writes or deletes
            -> restores within 35 days
            -> incremental backups
            -> not enable by default
            -> latest restorable = 5 minutes
    
    Stream - Time-ordered sequence of item-level changes in a table
        DynamoDB Stream (Shard(Stream Records))
            -> A DynamoDB(Shard1, Shard2, Shard3, Shardn)
                    Shard(StreamRecords1, StreamRecords2, StreamRecordsn)
        -> Store for 24 hours
        -> Insert, updates, and deletes
        -> combine with lambda
    
    Global Tables
        -> globally distributed applications
        -> based on DynamoDB streams
        -> muti-region redundancy for Disaster Recovery or HA
        -> no application rewrites
        -> replication latency under 1 second

    Database Migration Service (DMS)
        -> to migrate one database to another
        -> support Source database: on-prem, EC2, RDS, Aurora, S3, DB2, MariaDB, AzureDB, SQL Server
                                                    MongoDB, MySQL, Oracle, PostgreSQL, Sybase
        -> support Target database: on-prem, EC2, RDS, Aurora, DocumentDB, DynamoDB, Kinesis, Redshift,
                                                    S3, Elasticsearch, Kafka, MariaDB, SQL Server, MySQL
                                                    Oracle, PostgreSQL, Sybase
        -> source database remains operational
    
    Security
        -> KMS encrypt at rest
        -> Site-to-site VPN
        -> Direct Connect (DX)
        -> IAM policies roles
        -> Fine-grained access
        -> CloudWatch and CloudTrail
        -> VPC endpoints
            -> allow EC2 to connect to DynamoDB's private IP address
            -> no public exposure

Redshift - is AWS solution for Datawarehouse where there is a huge amount of data that need to be queried and processed
    Redshift configuration: Single Node vs Multi-Node
        -> Single node: 160GB
        -> Multi-Node: 
            + Leader Node: manages client connections and receives queries
            + Compute Nodes: store data + perform queries + computations, up to 128 Nodes

    Advanced Compression: automatically compress data

    Massively Parallel Processing (MPP):
        -> automatically distributes data and query load across all Nodes
        -> increase Compute Nodes to scale out Redshift
    
    *Backups
        -> Retention period: 1 day (default), 35 days max
        -> attemps to maintain at least 3 copies: origin + replica (on compute nodes) + a backup (on S3)
        -> can asynchronously replicate snapshots to S3 in another region (DR)

    Pricing
        -> Compute Node hours: total number of hours run across all compute nodes for 1 billing period
            For example: 3 nodes clusrer runs 30 days = 3 * 24 * 30 = 2160 instance hours
        -> charge backup
        -> charge data transfer (within VPC only, not outside)

    Security
        -> Intransit - encrypt SSL
        -> At rest - encrypt AES-256
        -> Redshift takes care of key management by default. Can be managed by:
            + your own keys through Hardware Security Module HSM
            + AWS KMS
    
    availability:
        -> only in 1 AZ
        -> can be restore to another AZ from a snapshot